{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# Welcome to your first assignment!\n",
    "\n",
    "Instructions are below. Please give this a try, and look in the solutions folder if you get stuck (or feel free to ask me!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada885d9-4d42-4d9b-97f0-74fbbbfe93a9",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Just before we get to the assignment --</h2>\n",
    "            <span style=\"color:#f71;\">I thought I'd take a second to point you at this page of useful resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458",
   "metadata": {},
   "source": [
    "# HOMEWORK EXERCISE ASSIGNMENT\n",
    "\n",
    "Upgrade the day 1 project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI\n",
    "\n",
    "You'll be able to use this technique for all subsequent projects if you'd prefer not to use paid APIs.\n",
    "\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
    "\n",
    "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code below from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ddd15d-a3c5-4f4e-a678-873f56162724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac0a679-599c-441f-9bf2-ddc73d35b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9c624-14f0-4945-a719-8ddb64f66f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ff514-e8bd-4985-a572-2ea28bb4fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9f644-522d-4e05-a691-56e7658c0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this doesn't work for any reason, try the 2 versions in the following cells\n",
    "# And double check the instructions in the 'Recap on installation of Ollama' at the top of this lab\n",
    "# And if none of that works - contact me!\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a021f13-d6a1-4b96-8e18-4eae49d876fe",
   "metadata": {},
   "source": [
    "# Introducing the ollama package\n",
    "\n",
    "And now we'll do the same thing, but using the elegant ollama python package instead of a direct HTTP call.\n",
    "\n",
    "Under the hood, it's making the same call as above to the ollama server running at localhost:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745b9c4-57dc-4867-9180-61fa5db55eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4704e10-f5fb-4c15-a935-f046c06fb13d",
   "metadata": {},
   "source": [
    "## Alternative approach - using OpenAI python library to connect to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23057e00-b6fc-4678-93a9-6b31cb704bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's actually an alternative approach that some people might prefer\n",
    "# You can use the OpenAI client python library to call Ollama:\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e22da-b891-41f6-9ac9-bd0c0a5f4f44",
   "metadata": {},
   "source": [
    "## Are you confused about why that works?\n",
    "\n",
    "It seems strange, right? We just used OpenAI code to call Ollama?? What's going on?!\n",
    "\n",
    "Here's the scoop:\n",
    "\n",
    "The python class `OpenAI` is simply code written by OpenAI engineers that makes calls over the internet to an endpoint.  \n",
    "\n",
    "When you call `openai.chat.completions.create()`, this python code just makes a web request to the following url: \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "Code like this is known as a \"client library\" - it's just wrapper code that runs on your machine to make web requests. The actual power of GPT is running on OpenAI's cloud behind this API, not on your computer!\n",
    "\n",
    "OpenAI was so popular, that lots of other AI providers provided identical web endpoints, so you could use the same approach.\n",
    "\n",
    "So Ollama has an endpoint running on your local box at http://localhost:11434/v1/chat/completions  \n",
    "And in week 2 we'll discover that lots of other providers do this too, including Gemini and DeepSeek.\n",
    "\n",
    "And then the team at OpenAI had a great idea: they can extend their client library so you can specify a different 'base url', and use their library to call any compatible API.\n",
    "\n",
    "That's it!\n",
    "\n",
    "So when you say: `ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')`  \n",
    "Then this will make the same endpoint calls, but to Ollama instead of OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d1de3-e2ac-46ff-a302-3b4ba38c4c90",
   "metadata": {},
   "source": [
    "## Also trying the amazing reasoning model DeepSeek\n",
    "\n",
    "Here we use the version of DeepSeek-reasoner that's been distilled to 1.5B.  \n",
    "This is actually a 1.5B variant of Qwen that has been fine-tuned using synethic data generated by Deepseek R1.\n",
    "\n",
    "Other sizes of DeepSeek are [here](https://ollama.com/library/deepseek-r1) all the way up to the full 671B parameter version, which would use up 404GB of your drive and is far too large for most!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eb44e-fe5b-47aa-b719-0bb63669ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d554b-e00d-4c08-9300-45e073950a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a few minutes to run! You should then see a fascinating \"thinking\" trace inside <think> tags, followed by some decent definitions\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Please give definitions of some core concepts behind LLMs: a neural network, attention and the transformer\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622d9bb-5c68-4d4e-9ca4-b492c751f898",
   "metadata": {},
   "source": [
    "# NOW the exercise for you\n",
    "\n",
    "Take the code from day1 and incorporate it here, to build a website summarizer that uses Llama 3.2 running locally instead of OpenAI; use either of the above approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6de38216-6d1c-48c4-877b-86d403f4e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API=\"http://localhost:11434/api/chat\"\n",
    "HEADERS={\"content-type\":\"application/json\"}\n",
    "MODEL=\"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cee338c0-1299-4f8e-ae93-0475de0636a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"You are an assistant who is excellent with summarizing content without losing the key idea. You are expected to generate a single subject line for the email.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf9ad924-2b41-496e-89ab-ebfec817ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt=\"\"\"\n",
    "Hi John, thanks again for all the work you’ve done as we near the beta \\\n",
    "launch of the VeggieVault recipe platform—I wanted to consolidate everything \\\n",
    "into one place before things get too chaotic. We’re still targeting July 5 for \\\n",
    "internal beta, and most stakeholder feedback has been addressed (around 80% complete), \\\n",
    "though a few remaining issues could impact demo polish: filters for “under 30 minutes” \\\n",
    "and “gluten-free” still don’t combine correctly, homepage hero image alt text is missing \\\n",
    "(flagged by accessibility), and the final count of imported recipes (goal is 10,000+) \\\n",
    "needs to be confirmed to fix the progress bar logic—can you make sure these are done \\\n",
    "by July 2 or let me know if we need to adjust the demo script? On the onboarding front, \\\n",
    "initial testers completed the recipe discovery tutorial successfully, but some UX issues \\\n",
    "popped up: people are confusing “bookmark recipe” with “add to pantry,” and it’s unclear \\\n",
    "whether saved pantry items are per-user or global (they are personalized)—maybe a tooltip \\\n",
    "would help? If design has time before July 1, it’d be great to fix it now; otherwise we’ll \\\n",
    "defer. I’ve also drafted a feedback survey for the ~25 internal testers and shared it in the \\\n",
    "Product Slack—please review it before EOD Friday. We should also plan a post-beta analytics \\\n",
    "and roadmap sync with Product, UX, and Eng—suggesting July 10 or 11—can you confirm your \\\n",
    "availability? Meanwhile, huge props to you and the data team for improving ingredient \\\n",
    "taxonomy; early SEO tests are encouraging. One note: on several recipe pages, “vegan” \\\n",
    "and “vegetarian” tags appear together even when dairy is present—let’s add a validation \\\n",
    "check for conflicting tags before recipes are published. Also, a few imported recipes \\\n",
    "(like from BBC GoodFood) are missing images—can we define a fallback strategy? I’ll \\\n",
    "update the project board tonight with open tasks and dependencies. Let’s stay close \\\n",
    "on Slack this week—appreciate everything, we’re almost there!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78ed6a45-f586-4e95-bcab-7e3fd50e6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\":\"user\", \"content\":user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5761e22-442f-44d2-941c-6bddd081ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"model\":MODEL,\n",
    "    \"messages\":messages,\n",
    "    \"stream\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "346218ac-935a-4d6b-ac43-537b553f66f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "848736c2-f55a-4f3b-85b1-80b7070c565c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an email to John, addressing several issues and tasks related to the VeggieVault recipe platform's beta launch. Here are some key points:\n",
      "\n",
      "1. The deadline for internal beta is still July 5, but a few issues need to be addressed: \n",
      "   - Filters for \"under 30 minutes\" and \"gluten-free\" don't combine correctly.\n",
      "   - Homepage hero image alt text is missing (flagged by accessibility).\n",
      "   - Final count of imported recipes needs to be confirmed.\n",
      "\n",
      "2. For the onboarding, there are a couple of UX issues:\n",
      "   - Users are confusing \"bookmark recipe\" with \"add to pantry.\"\n",
      "   - It's unclear whether saved pantry items are per-user or global.\n",
      "   A tooltip could help clarify this.\n",
      "\n",
      "3. The developer team has drafted a feedback survey for internal testers and shared it in the Product Slack channel.\n",
      "\n",
      "4. They plan an analytics and roadmap sync for July 10 or 11, which will include representatives from Product, UX, and Eng.\n",
      "\n",
      "5. There's also a suggestion to add a validation check for conflicting tags (\"vegan\" and \"vegetarian\") before publishing recipes.\n",
      "   Additionally, they need to define a fallback strategy for imported recipes that are missing images (for example, those from BBC GoodFood).\n",
      "\n",
      "Overall, it seems like there is still some work left to be done before the beta launch on July 5.\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2b0c14b-b8f7-4a87-93a5-d37533b943dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0faa80f2-534e-46d5-87f1-df3fd26965ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a response to your email:\n",
      "\n",
      "Hi [Name],\n",
      "\n",
      "Thanks for the detailed summary of our progress leading up to the internal beta launch! I'm glad to hear that 80% of stakeholder feedback has been addressed, and I'll make sure to focus on the remaining issues by July 2.\n",
      "\n",
      "Regarding the filters, I've taken a look at the code and believe we can fix the combination issue for \"under 30 minutes\" and \"gluten-free\". However, please confirm that this is acceptable, or we may need to adjust the demo script accordingly. For the homepage hero image alt text, I'll add it immediately to address accessibility concerns.\n",
      "\n",
      "For the final count of imported recipes, I've checked on our progress, and we're currently at 8,500+ recipes. We should be able to reach the goal of 10,000 by July 5. If this changes, please let me know, and we'll adjust the progress bar logic accordingly.\n",
      "\n",
      "On the onboarding front, I agree that a tooltip would help clarify the difference between \"bookmark recipe\" and \"add to pantry\". I'll make sure to prioritize this issue if design has time before July 1; otherwise, we can consider deferring it. The UX issues are indeed puzzling, especially regarding saved pantry items.\n",
      "\n",
      "I've reviewed your draft feedback survey for internal testers and appreciate the work you put into creating it. Please share any updates or revisions with me by EOD Friday.\n",
      "\n",
      "For our post-beta analytics and roadmap sync, I'm available on July 10 or 11. Which day works best for everyone?\n",
      "\n",
      "Lastly, I'll support adding a validation check for conflicting tags (\"vegan\" and \"vegetarian\") to prevent recipes from being published incorrectly. Also, I agree that we should define a fallback strategy for missing images in imported recipes.\n",
      "\n",
      "Please update the project board with open tasks and dependencies tonight. I'll stay close on Slack this week, and thank you again for your efforts. We're almost there!\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21294497-c7b9-44e9-8cd8-41f77c9a09bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
